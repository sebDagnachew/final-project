{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sebDagnachew/final-project-object-detection/blob/main/LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sentencepiece as spm  #imports the SentencePiece library,\n",
        "import time  #import time\n",
        "\n",
        "# ---------- 1. Train SentencePiece tokenizer ----------\n",
        "corpus_file = '/content/1. What is a Patent.txt'   # your Amharic corpus file\n",
        "model_prefix = 'amharic_sp'\n",
        "vocab_size = 400                  # can also try 500\n",
        "\n",
        "print(\"Training SentencePiece tokenizer...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=corpus_file,\n",
        "    model_prefix=model_prefix,\n",
        "    vocab_size=vocab_size,\n",
        "    model_type='unigram',       # recommended for Amharic\n",
        "    character_coverage=1.0,  #Tells SentencePiece to include all characters from your text\n",
        "    pad_id=0,  #padding\n",
        "    unk_id=1,  #unknown words\n",
        "    bos_id=2,  #beginning of sentence\n",
        "    eos_id=3   #end of sentence\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Training completed in {training_time:.2f} seconds.\\n\")\n",
        "\n",
        "# ---------- 2. Load the trained model ----------\n",
        "sp = spm.SentencePieceProcessor(model_file=f'{model_prefix}.model')\n",
        "\n",
        "# ---------- 3. Read corpus and tokenize ----------\n",
        "with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "total_words = 0\n",
        "total_tokens = 0\n",
        "subword_fertility_list = []\n",
        "continued_words_count = 0  # For PCW\n",
        "\n",
        "tokenization_start = time.time()\n",
        "\n",
        "for line in lines:\n",
        "    line = line.strip() #removes extra spaces or newline characters.\n",
        "    if not line:\n",
        "        continue\n",
        "    words = line.split()  # split by space\n",
        "    total_words += len(words)\n",
        "\n",
        "    tokens = sp.encode(line, out_type=str)\n",
        "    total_tokens += len(tokens)\n",
        "\n",
        "    # Subword fertility per line\n",
        "    if len(words) > 0:\n",
        "        subword_fertility_list.append(len(tokens)/len(words))\n",
        "\n",
        "    # Count continued words\n",
        "    # Assume a word is continued if it is split into more than 1 token\n",
        "    # Simple method: check token boundaries for each word\n",
        "    token_index = 0\n",
        "    for word in words:\n",
        "        word_tokens = sp.encode(word, out_type=str) # if someone wants token_id [out_type=int]\n",
        "        if len(word_tokens) > 1:\n",
        "            continued_words_count += 1\n",
        "        token_index += len(word_tokens)\n",
        "\n",
        "tokenization_time = time.time() - tokenization_start\n",
        "\n",
        "# ---------- 4. Calculate metrics ----------\n",
        "normalized_sequence_length = total_tokens / total_words\n",
        "average_subword_fertility = sum(subword_fertility_list) / len(subword_fertility_list)\n",
        "proportion_continued_words = continued_words_count / total_words\n",
        "\n",
        "print(\"----- Tokenization Metrics -----\")\n",
        "print(f\"Total words in corpus: {total_words}\")\n",
        "print(f\"Total tokens produced: {total_tokens}\")\n",
        "print(f\"Normalized Sequence Length (tokens/word): {normalized_sequence_length:.3f}\")\n",
        "print(f\"Average Subword Fertility: {average_subword_fertility:.3f}\")\n",
        "print(f\"Proportion of Continued Words (PCW): {proportion_continued_words:.3f}\")\n",
        "print(f\"Tokenization execution time: {tokenization_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFAXbYuBFstQ",
        "outputId": "0598196c-2a54-49be-996d-7cc42d68d34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SentencePiece tokenizer...\n",
            "Training completed in 0.03 seconds.\n",
            "\n",
            "----- Tokenization Metrics -----\n",
            "Total words in corpus: 607\n",
            "Total tokens produced: 1645\n",
            "Normalized Sequence Length (tokens/word): 2.710\n",
            "Average Subword Fertility: 2.802\n",
            "Proportion of Continued Words (PCW): 0.764\n",
            "Tokenization execution time: 0.01 seconds\n"
          ]
        }
      ]
    }
  ]
}